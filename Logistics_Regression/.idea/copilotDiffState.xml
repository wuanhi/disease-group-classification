<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/ChatBotUsingLogisticRegression.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/ChatBotUsingLogisticRegression.py" />
              <option name="originalContent" value="import re&#10;import subprocess&#10;import sys&#10;import requests&#10;from sklearn.feature_extraction.text import TfidfVectorizer&#10;from underthesea import word_tokenize&#10;import pandas as pd&#10;from sklearn.preprocessing import LabelEncoder&#10;from sklearn.linear_model import LogisticRegression&#10;from sklearn.metrics import classification_report, accuracy_score&#10;import evaluate&#10;&#10;# Cai dat thu vien&#10;# def install_libraries(libraries):&#10;#     for lib in libraries:&#10;#         try:&#10;#             print(f&quot;Đang cài đặt {lib}...&quot;)&#10;#             subprocess.check_call([sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;-q&quot;, lib])&#10;#             print(f&quot;Đã cài đặt thành công {lib}&quot;)&#10;#         except subprocess.CalledProcessError as e:&#10;#             print(f&quot;Lỗi khi cài đặt {lib}: {e}&quot;)&#10;#         except Exception as e:&#10;#             print(f&quot;Có lỗi xảy ra: {e}&quot;)&#10;&#10;# Install required libraries first&#10;# required_libraries = ['pandas', 'numpy', 'underthesea','fsspec','huggingface_hub', 'evaluate', 'requests', 'scikit-learn']&#10;# print(&quot;Installing required libraries...&quot;)&#10;# install_libraries(required_libraries)&#10;&#10;# Load dataset&#10;def dataloader(path):&#10;    data = pd.read_csv(path)&#10;    return data&#10;&#10;# Xu ly dataset&#10;# Vector du lieu bang TfidfVectorizer&#10;def encoder(data, column_name, encoded_label_colname, ecr):&#10;    data[encoded_label_colname] = ecr.fit_transform(data[column_name])&#10;    return data&#10;&#10;# Lay stopword tu github&#10;response = requests.get(&quot;https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt&quot;)&#10;vietnamese_stopwords = set(response.text.splitlines())&#10;&#10;# Tien xu ly van ban&#10;def process_text(text):&#10;    if isinstance(text, str):&#10;        # Loai bo cac ky tu dac biet&#10;        text = re.sub(r'[^\w\s]', '', text)&#10;        # Chuyen ve chu thuong&#10;        text = text.lower()&#10;        # Tach tu bang underthesea&#10;        text_tokenized = word_tokenize(text, format=&quot;text&quot;)&#10;        # Loai bo cac stop word&#10;        words = text_tokenized.split()&#10;        filtered_words = [word for word in words if word not in vietnamese_stopwords]&#10;        return &quot; &quot;.join(filtered_words)&#10;    return &quot;&quot;&#10;&#10;&#10;def process_data(data, label_colname, feature_colname, processed_feature_colname):&#10;    # Loai bo cac dong du lieu trung lap&#10;    data.drop_duplicates(inplace=True)&#10;    # Loai bo cac dong du lieu bi thieu&#10;    data = data.dropna(subset=[feature_colname, label_colname])&#10;    # Xu ly du lieu&#10;    data[processed_feature_colname] = data[feature_colname].apply(process_text)&#10;    return data&#10;&#10;def vectorizer_data(data, processed_feature_colname, vtr):&#10;    vector = vtr.fit_transform(data[processed_feature_colname])&#10;    return vector&#10;&#10;&#10;# Huan luyen mo hinh&#10;def load_model(X, Y):&#10;    md = LogisticRegression(random_state=42, max_iter=1000)&#10;    md.fit(X, Y)&#10;    return md&#10;&#10;&#10;# Thu mo hinh thuc te&#10;def predict(input_data, md, vtr, ecr):&#10;    ip_processed = process_text(input_data)&#10;    ip_vec = vtr.transform([ip_processed])&#10;    predicted_label_index = md.predict(ip_vec)[0]&#10;    predicted = ecr.inverse_transform([predicted_label_index])[0]&#10;    return predicted&#10;&#10;&#10;def show_result(input_data, result):&#10;    print(f&quot;Câu hỏi: {input_data}&quot;)&#10;    print(f&quot;Dự đoán bệnh: {result}&quot;)&#10;    print(&quot;-&quot; * 30)&#10;&#10;# Kiem thu mo hinh&#10;def validate_model_using_human(md, vtr, ecr, data, feature_colname):&#10;    for items in data[feature_colname]:&#10;        show_result(items, predict(items, md, vtr, ecr))&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # Load dataset&#10;    df = dataloader(&quot;hf://datasets/joon985/ViMedical_Disease_Category/ViMedicalDiseaseCategory.csv&quot;)&#10;    print(&quot;Dataset loaded successfully!&quot;)&#10;    print(f&quot;Dataset shape: {df.shape}&quot;)&#10;    print(&quot;\nFirst few rows:&quot;)&#10;    print(df.head())&#10;&#10;    # Check available columns&#10;    print(f&quot;\nColumns in dataset: {df.columns.tolist()}&quot;)&#10;&#10;    # Initialize encoders and vectorizer&#10;    label_encoder = LabelEncoder()&#10;    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))&#10;&#10;    # Sử dụng tên cột đúng với dataset&#10;    target_column = 'DiseaseCategory'  # Tên cột nhóm bệnh&#10;    feature_column = 'Question'  # Tên cột mô tả triệu chứng&#10;&#10;    # Kiểm tra cân bằng dữ liệu&#10;    print(f&quot;\nPhân bố dữ liệu theo nhóm bệnh:&quot;)&#10;    print(df[target_column].value_counts())&#10;&#10;    # Encode target labels (disease groups)&#10;    df = encoder(df, target_column, f'{target_column}_encoded', label_encoder)&#10;&#10;    # Process text data&#10;    df = process_data(df, target_column, feature_column, f'{feature_column}_processed')&#10;&#10;    # Vectorize features&#10;    X_vec = vectorizer_data(df, f&quot;{feature_column}_processed&quot;, vectorizer)&#10;&#10;    # Chia dữ liệu thành train/validation/test (80/10/10)&#10;    from sklearn.model_selection import train_test_split&#10;&#10;    # Chia đầu tiên: 80% train, 20% temp (sẽ chia tiếp thành val và test)&#10;    X_train, X_temp, y_train, y_temp = train_test_split(&#10;        X_vec, df[f&quot;{target_column}_encoded&quot;],&#10;        test_size=0.2,&#10;        random_state=42,&#10;        stratify=df[f&quot;{target_column}_encoded&quot;]&#10;    )&#10;&#10;    # Chia 20% temp thành 10% validation và 10% test&#10;    X_val, X_test, y_val, y_test = train_test_split(&#10;        X_temp, y_temp,&#10;        test_size=0.5,&#10;        random_state=42,&#10;        stratify=y_temp&#10;    )&#10;&#10;    print(f&quot;\nKích thước tập dữ liệu:&quot;)&#10;    print(f&quot;Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(df)*100:.1f}%)&quot;)&#10;    print(f&quot;Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(df)*100:.1f}%)&quot;)&#10;    print(f&quot;Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(df)*100:.1f}%)&quot;)&#10;&#10;    # Xử lý dữ liệu mất cân bằng bằng class_weight&#10;    from sklearn.utils.class_weight import compute_class_weight&#10;    import numpy as np&#10;&#10;    # Tính toán class weights để xử lý imbalanced data&#10;    classes = np.unique(y_train)&#10;    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)&#10;    class_weight_dict = dict(zip(classes, class_weights))&#10;&#10;    print(f&quot;\nClass weights để xử lý dữ liệu mất cân bằng:&quot;)&#10;    for class_id, weight in class_weight_dict.items():&#10;        class_name = label_encoder.inverse_transform([class_id])[0]&#10;        print(f&quot;{class_name}: {weight:.3f}&quot;)&#10;&#10;    # Train model với class_weight&#10;    print(&quot;\nTraining logistic regression model with balanced class weights...&quot;)&#10;    from sklearn.linear_model import LogisticRegression&#10;    from sklearn.ensemble import RandomForestClassifier, VotingClassifier&#10;    from sklearn.svm import SVC&#10;    from sklearn.model_selection import GridSearchCV&#10;    from sklearn.pipeline import Pipeline&#10;    from sklearn.feature_selection import SelectKBest, chi2&#10;&#10;    # 1. FEATURE SELECTION - Chọn các features quan trọng nhất&#10;    print(&quot;Performing feature selection...&quot;)&#10;    feature_selector = SelectKBest(chi2, k=3000)  # Chọn 3000 features tốt nhất&#10;    X_train_selected = feature_selector.fit_transform(X_train, y_train)&#10;    X_val_selected = feature_selector.transform(X_val)&#10;    X_test_selected = feature_selector.transform(X_test)&#10;&#10;    # 2. HYPERPARAMETER TUNING cho Logistic Regression&#10;    print(&quot;Hyperparameter tuning for Logistic Regression...&quot;)&#10;    lr_param_grid = {&#10;        'C': [0.1, 1, 10, 100],&#10;        'solver': ['liblinear', 'lbfgs'],&#10;        'max_iter': [1000, 2000]&#10;    }&#10;&#10;    lr_grid = GridSearchCV(&#10;        LogisticRegression(class_weight='balanced', random_state=42),&#10;        lr_param_grid,&#10;        cv=3,&#10;        scoring='accuracy',&#10;        n_jobs=-1&#10;    )&#10;    lr_grid.fit(X_train_selected, y_train)&#10;&#10;    print(f&quot;Best LR parameters: {lr_grid.best_params_}&quot;)&#10;    print(f&quot;Best LR CV score: {lr_grid.best_score_:.4f}&quot;)&#10;&#10;    # 3. ENSEMBLE METHODS - Kết hợp nhiều models&#10;    print(&quot;Training ensemble models...&quot;)&#10;&#10;    # Random Forest&#10;    rf_model = RandomForestClassifier(&#10;        n_estimators=200,&#10;        max_depth=20,&#10;        min_samples_split=5,&#10;        min_samples_leaf=2,&#10;        class_weight='balanced',&#10;        random_state=42,&#10;        n_jobs=-1&#10;    )&#10;&#10;    # SVM&#10;    svm_model = SVC(&#10;        C=10,&#10;        kernel='rbf',&#10;        class_weight='balanced',&#10;        probability=True,&#10;        random_state=42&#10;    )&#10;&#10;    # Voting Classifier (kết hợp 3 models)&#10;    ensemble_model = VotingClassifier(&#10;        estimators=[&#10;            ('lr', lr_grid.best_estimator_),&#10;            ('rf', rf_model),&#10;            ('svm', svm_model)&#10;        ],&#10;        voting='soft'&#10;    )&#10;&#10;    # Train tất cả models&#10;    print(&quot;Training individual models...&quot;)&#10;    lr_grid.best_estimator_.fit(X_train_selected, y_train)&#10;    rf_model.fit(X_train_selected, y_train)&#10;    svm_model.fit(X_train_selected, y_train)&#10;    ensemble_model.fit(X_train_selected, y_train)&#10;&#10;    # 4. ĐÁNH GIÁ TỪNG MODEL&#10;    models = {&#10;        'Optimized Logistic Regression': lr_grid.best_estimator_,&#10;        'Random Forest': rf_model,&#10;        'SVM': svm_model,&#10;        'Ensemble (Voting)': ensemble_model&#10;    }&#10;&#10;    best_model = None&#10;    best_accuracy = 0&#10;&#10;    print(&quot;\n&quot; + &quot;=&quot;*60)&#10;    print(&quot;COMPARING ALL MODELS&quot;)&#10;    print(&quot;=&quot;*60)&#10;&#10;    for name, model in models.items():&#10;        # Validation accuracy&#10;        y_val_pred = model.predict(X_val_selected)&#10;        val_acc = accuracy_score(y_val, y_val_pred)&#10;&#10;        # Test accuracy&#10;        y_test_pred = model.predict(X_test_selected)&#10;        test_acc = accuracy_score(y_test, y_test_pred)&#10;&#10;        print(f&quot;\n{name}:&quot;)&#10;        print(f&quot;  Validation Accuracy: {val_acc:.4f}&quot;)&#10;        print(f&quot;  Test Accuracy: {test_acc:.4f}&quot;)&#10;&#10;        if val_acc &gt; best_accuracy:&#10;            best_accuracy = val_acc&#10;            best_model = model&#10;&#10;    print(f&quot;\nBest model: {type(best_model).__name__}&quot;)&#10;    print(f&quot;Best validation accuracy: {best_accuracy:.4f}&quot;)&#10;&#10;    # 5. TỐI ƯU THÊM VỚI TF-IDF PARAMETERS&#10;    print(&quot;\n&quot; + &quot;=&quot;*60)&#10;    print(&quot;OPTIMIZING TF-IDF PARAMETERS&quot;)&#10;    print(&quot;=&quot;*60)&#10;&#10;    # Thử nghiệm với các tham số TF-IDF khác nhau&#10;    from sklearn.feature_extraction.text import TfidfVectorizer&#10;&#10;    tfidf_configs = [&#10;        {'max_features': 8000, 'ngram_range': (1, 2), 'min_df': 2, 'max_df': 0.95},&#10;        {'max_features': 10000, 'ngram_range': (1, 3), 'min_df': 3, 'max_df': 0.9},&#10;        {'max_features': 12000, 'ngram_range': (1, 2), 'min_df': 2, 'max_df': 0.98, 'sublinear_tf': True}&#10;    ]&#10;&#10;    best_tfidf_accuracy = 0&#10;    best_tfidf_config = None&#10;    best_tfidf_model = None&#10;&#10;    for i, config in enumerate(tfidf_configs):&#10;        print(f&quot;\nTesting TF-IDF config {i+1}: {config}&quot;)&#10;&#10;        # Tạo vectorizer mới với config này&#10;        new_vectorizer = TfidfVectorizer(**config)&#10;&#10;        # Vectorize lại dữ liệu đã process&#10;        X_new = new_vectorizer.fit_transform(df[f&quot;{feature_column}_processed&quot;])&#10;&#10;        # Chia lại dữ liệu&#10;        X_train_new, X_temp_new, y_train_new, y_temp_new = train_test_split(&#10;            X_new, df[f&quot;{target_column}_encoded&quot;],&#10;            test_size=0.2,&#10;            random_state=42,&#10;            stratify=df[f&quot;{target_column}_encoded&quot;]&#10;        )&#10;&#10;        X_val_new, X_test_new, y_val_new, y_test_new = train_test_split(&#10;            X_temp_new, y_temp_new,&#10;            test_size=0.5,&#10;            random_state=42,&#10;            stratify=y_temp_new&#10;        )&#10;&#10;        # Feature selection&#10;        selector_new = SelectKBest(chi2, k=min(5000, X_train_new.shape[1]))&#10;        X_train_selected_new = selector_new.fit_transform(X_train_new, y_train_new)&#10;        X_val_selected_new = selector_new.transform(X_val_new)&#10;&#10;        # Train best model type với config mới&#10;        test_model = RandomForestClassifier(&#10;            n_estimators=300,&#10;            max_depth=25,&#10;            min_samples_split=3,&#10;            min_samples_leaf=1,&#10;            class_weight='balanced',&#10;            random_state=42,&#10;            n_jobs=-1&#10;        )&#10;&#10;        test_model.fit(X_train_selected_new, y_train_new)&#10;        val_pred = test_model.predict(X_val_selected_new)&#10;        val_accuracy = accuracy_score(y_val_new, val_pred)&#10;&#10;        print(f&quot;  Validation accuracy: {val_accuracy:.4f}&quot;)&#10;&#10;        if val_accuracy &gt; best_tfidf_accuracy:&#10;            best_tfidf_accuracy = val_accuracy&#10;            best_tfidf_config = config&#10;            best_tfidf_model = test_model&#10;            # Lưu vectorizer và selector tốt nhất&#10;            best_vectorizer = new_vectorizer&#10;            best_selector = selector_new&#10;            best_X_test = X_test_new&#10;            best_y_test = y_test_new&#10;            best_X_test_selected = selector_new.transform(X_test_new)&#10;&#10;    print(f&quot;\nBest TF-IDF config: {best_tfidf_config}&quot;)&#10;    print(f&quot;Best TF-IDF validation accuracy: {best_tfidf_accuracy:.4f}&quot;)&#10;&#10;    # Test accuracy với config tốt nhất&#10;    if best_tfidf_model:&#10;        final_test_pred = best_tfidf_model.predict(best_X_test_selected)&#10;        final_test_accuracy = accuracy_score(best_y_test, final_test_pred)&#10;&#10;        print(f&quot;\nFINAL OPTIMIZED RESULTS:&quot;)&#10;        print(f&quot;Final Test Accuracy: {final_test_accuracy:.4f}&quot;)&#10;        print(f&quot;Improvement: {final_test_accuracy - 0.694:.4f} (+{((final_test_accuracy - 0.694)/0.694)*100:.1f}%)&quot;)&#10;&#10;        # Sử dụng model tốt nhất cho prediction&#10;        model = best_tfidf_model&#10;        vectorizer = best_vectorizer&#10;        feature_selector = best_selector&#10;    else:&#10;        model = best_model&#10;        feature_selector = feature_selector&#10;&#10;    # 6. CẬP NHẬT PREDICTION FUNCTION&#10;    def predict_disease_group(input_data, md, vtr, ecr, selector=None):&#10;        ip_processed = process_text(input_data)&#10;        ip_vec = vtr.transform([ip_processed])&#10;&#10;        if selector:&#10;            ip_vec = selector.transform(ip_vec)&#10;&#10;        predicted_label_index = md.predict(ip_vec)[0]&#10;        predicted_group = ecr.inverse_transform([predicted_label_index])[0]&#10;&#10;        # Get prediction probability for confidence&#10;        probabilities = md.predict_proba(ip_vec)[0]&#10;        confidence = max(probabilities)&#10;&#10;        return predicted_group, confidence&#10;&#10;    def show_group_result(input_data, result, confidence):&#10;        print(f&quot;Mô tả triệu chứng: {input_data}&quot;)&#10;        print(f&quot;Dự đoán nhóm bệnh: {result}&quot;)&#10;        print(f&quot;Độ tin cậy: {confidence:.2%}&quot;)&#10;        print(&quot;-&quot; * 50)&#10;&#10;    # Test with some sample predictions&#10;    print(&quot;\n&quot; + &quot;=&quot;*60)&#10;    print(&quot;TESTING OPTIMIZED DISEASE GROUP PREDICTION&quot;)&#10;    print(&quot;=&quot;*60)&#10;&#10;    # Interactive testing với model đã tối ưu&#10;    while True:&#10;        user_input = input(&quot;\nNhập mô tả triệu chứng (hoặc 'quit' để thoát): &quot;)&#10;        if user_input.lower() in ['quit', 'exit', 'thoát']:&#10;            break&#10;&#10;        predicted_group, confidence = predict_disease_group(&#10;            user_input, model, vectorizer, label_encoder,&#10;            selector=feature_selector if 'feature_selector' in locals() else None&#10;        )&#10;        show_group_result(user_input, predicted_group, confidence)&#10;&#10;        # Show top 3 probable groups với model tối ưu&#10;        ip_processed = process_text(user_input)&#10;        ip_vec = vectorizer.transform([ip_processed])&#10;&#10;        if 'feature_selector' in locals():&#10;            ip_vec = feature_selector.transform(ip_vec)&#10;&#10;        probabilities = model.predict_proba(ip_vec)[0]&#10;        top_indices = probabilities.argsort()[-3:][::-1]&#10;&#10;        print(&quot;Top 3 nhóm bệnh có khả năng cao nhất:&quot;)&#10;        for i, idx in enumerate(top_indices, 1):&#10;            group_name = label_encoder.inverse_transform([idx])[0]&#10;            prob = probabilities[idx]&#10;            print(f&quot;{i}. {group_name}: {prob:.2%}&quot;)&#10;&#10;    print(&quot;\nCảm ơn bạn đã sử dụng hệ thống dự đoán nhóm bệnh tối ưu!&quot;)&#10;" />
              <option name="updatedContent" value="import re&#10;import subprocess&#10;import sys&#10;import requests&#10;from sklearn.feature_extraction.text import TfidfVectorizer&#10;from underthesea import word_tokenize&#10;import pandas as pd&#10;from sklearn.preprocessing import LabelEncoder&#10;from sklearn.linear_model import LogisticRegression&#10;from sklearn.metrics import classification_report, accuracy_score&#10;import evaluate&#10;&#10;# Cai dat thu vien&#10;# def install_libraries(libraries):&#10;#     for lib in libraries:&#10;#         try:&#10;#             print(f&quot;Đang cài đặt {lib}...&quot;)&#10;#             subprocess.check_call([sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;-q&quot;, lib])&#10;#             print(f&quot;Đã cài đặt thành công {lib}&quot;)&#10;#         except subprocess.CalledProcessError as e:&#10;#             print(f&quot;Lỗi khi cài đặt {lib}: {e}&quot;)&#10;#         except Exception as e:&#10;#             print(f&quot;Có lỗi xảy ra: {e}&quot;)&#10;&#10;# Install required libraries first&#10;# required_libraries = ['pandas', 'numpy', 'underthesea','fsspec','huggingface_hub', 'evaluate', 'requests', 'scikit-learn']&#10;# print(&quot;Installing required libraries...&quot;)&#10;# install_libraries(required_libraries)&#10;&#10;# Load dataset&#10;def dataloader(path):&#10;    data = pd.read_csv(path)&#10;    return data&#10;&#10;# Xu ly dataset&#10;# Vector du lieu bang TfidfVectorizer&#10;def encoder(data, column_name, encoded_label_colname, ecr):&#10;    data[encoded_label_colname] = ecr.fit_transform(data[column_name])&#10;    return data&#10;&#10;# Lay stopword tu github&#10;response = requests.get(&quot;https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt&quot;)&#10;vietnamese_stopwords = set(response.text.splitlines())&#10;&#10;# Tien xu ly van ban&#10;def process_text(text):&#10;    if isinstance(text, str):&#10;        # Loai bo cac ky tu dac biet&#10;        text = re.sub(r'[^\w\s]', '', text)&#10;        # Chuyen ve chu thuong&#10;        text = text.lower()&#10;        # Tach tu bang underthesea&#10;        text_tokenized = word_tokenize(text, format=&quot;text&quot;)&#10;        # Loai bo cac stop word&#10;        words = text_tokenized.split()&#10;        filtered_words = [word for word in words if word not in vietnamese_stopwords]&#10;        return &quot; &quot;.join(filtered_words)&#10;    return &quot;&quot;&#10;&#10;&#10;def process_data(data, label_colname, feature_colname, processed_feature_colname):&#10;    # Loai bo cac dong du lieu trung lap&#10;    data.drop_duplicates(inplace=True)&#10;    # Loai bo cac dong du lieu bi thieu&#10;    data = data.dropna(subset=[feature_colname, label_colname])&#10;    # Xu ly du lieu&#10;    data[processed_feature_colname] = data[feature_colname].apply(process_text)&#10;    return data&#10;&#10;def vectorizer_data(data, processed_feature_colname, vtr):&#10;    vector = vtr.fit_transform(data[processed_feature_colname])&#10;    return vector&#10;&#10;&#10;# Huan luyen mo hinh&#10;def load_model(X, Y):&#10;    md = LogisticRegression(random_state=42, max_iter=1000)&#10;    md.fit(X, Y)&#10;    return md&#10;&#10;&#10;# Thu mo hinh thuc te&#10;def predict(input_data, md, vtr, ecr):&#10;    ip_processed = process_text(input_data)&#10;    ip_vec = vtr.transform([ip_processed])&#10;    predicted_label_index = md.predict(ip_vec)[0]&#10;    predicted = ecr.inverse_transform([predicted_label_index])[0]&#10;    return predicted&#10;&#10;&#10;def show_result(input_data, result):&#10;    print(f&quot;Câu hỏi: {input_data}&quot;)&#10;    print(f&quot;Dự đoán bệnh: {result}&quot;)&#10;    print(&quot;-&quot; * 30)&#10;&#10;# Kiem thu mo hinh&#10;def validate_model_using_human(md, vtr, ecr, data, feature_colname):&#10;    for items in data[feature_colname]:&#10;        show_result(items, predict(items, md, vtr, ecr))&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # Load dataset&#10;    df = dataloader(&quot;hf://datasets/joon985/ViMedical_Disease_Category/ViMedicalDiseaseCategory.csv&quot;)&#10;    print(&quot;Dataset loaded successfully!&quot;)&#10;    print(f&quot;Dataset shape: {df.shape}&quot;)&#10;    print(&quot;\nFirst few rows:&quot;)&#10;    print(df.head())&#10;&#10;    # Check available columns&#10;    print(f&quot;\nColumns in dataset: {df.columns.tolist()}&quot;)&#10;&#10;    # Initialize encoders and vectorizer&#10;    label_encoder = LabelEncoder()&#10;    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))&#10;&#10;    # Sử dụng tên cột đúng với dataset&#10;    target_column = 'DiseaseCategory'  # Tên cột nhóm bệnh&#10;    feature_column = 'Question'  # Tên cột mô tả triệu chứng&#10;&#10;    # Kiểm tra cân bằng dữ liệu&#10;    print(f&quot;\nPhân bố dữ liệu theo nhóm bệnh:&quot;)&#10;    print(df[target_column].value_counts())&#10;&#10;    # Encode target labels (disease groups)&#10;    df = encoder(df, target_column, f'{target_column}_encoded', label_encoder)&#10;&#10;    # Process text data&#10;    df = process_data(df, target_column, feature_column, f'{feature_column}_processed')&#10;&#10;    # Vectorize features&#10;    X_vec = vectorizer_data(df, f&quot;{feature_column}_processed&quot;, vectorizer)&#10;&#10;    # Chia dữ liệu thành train/validation/test (80/10/10)&#10;    from sklearn.model_selection import train_test_split&#10;&#10;    # Chia đầu tiên: 80% train, 20% temp (sẽ chia tiếp thành val và test)&#10;    X_train, X_temp, y_train, y_temp = train_test_split(&#10;        X_vec, df[f&quot;{target_column}_encoded&quot;],&#10;        test_size=0.2,&#10;        random_state=42,&#10;        stratify=df[f&quot;{target_column}_encoded&quot;]&#10;    )&#10;&#10;    # Chia 20% temp thành 10% validation và 10% test&#10;    X_val, X_test, y_val, y_test = train_test_split(&#10;        X_temp, y_temp,&#10;        test_size=0.5,&#10;        random_state=42,&#10;        stratify=y_temp&#10;    )&#10;&#10;    print(f&quot;\nKích thước tập dữ liệu:&quot;)&#10;    print(f&quot;Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(df)*100:.1f}%)&quot;)&#10;    print(f&quot;Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(df)*100:.1f}%)&quot;)&#10;    print(f&quot;Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(df)*100:.1f}%)&quot;)&#10;&#10;    # Xử lý dữ liệu mất cân bằng bằng class_weight&#10;    from sklearn.utils.class_weight import compute_class_weight&#10;    import numpy as np&#10;&#10;    # Tính toán class weights để xử lý imbalanced data&#10;    classes = np.unique(y_train)&#10;    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)&#10;    class_weight_dict = dict(zip(classes, class_weights))&#10;&#10;    print(f&quot;\nClass weights để xử lý dữ liệu mất cân bằng:&quot;)&#10;    for class_id, weight in class_weight_dict.items():&#10;        class_name = label_encoder.inverse_transform([class_id])[0]&#10;        print(f&quot;{class_name}: {weight:.3f}&quot;)&#10;&#10;    # Train model với class_weight - SIMPLIFIED OPTIMIZATION&#10;    print(&quot;\nTraining optimized models...&quot;)&#10;    from sklearn.linear_model import LogisticRegression&#10;    from sklearn.ensemble import RandomForestClassifier&#10;    from sklearn.model_selection import GridSearchCV&#10;    from sklearn.feature_selection import SelectKBest, chi2&#10;    &#10;    # 1. BASELINE MODEL (để so sánh)&#10;    baseline_model = LogisticRegression(&#10;        random_state=42, &#10;        max_iter=1000,&#10;        class_weight='balanced'&#10;    )&#10;    baseline_model.fit(X_train, y_train)&#10;    baseline_val_pred = baseline_model.predict(X_val)&#10;    baseline_accuracy = accuracy_score(y_val, baseline_val_pred)&#10;    &#10;    print(f&quot;Baseline accuracy: {baseline_accuracy:.4f}&quot;)&#10;    &#10;    # 2. TỐI ƯU TF-IDF ĐƠN GIẢN&#10;    print(&quot;Optimizing TF-IDF parameters...&quot;)&#10;    &#10;    # Test các cấu hình TF-IDF đơn giản hơn&#10;    tfidf_configs = [&#10;        {'max_features': 7000, 'ngram_range': (1, 2), 'min_df': 2, 'max_df': 0.95},&#10;        {'max_features': 8000, 'ngram_range': (1, 2), 'min_df': 3, 'max_df': 0.9},&#10;        {'max_features': 6000, 'ngram_range': (1, 3), 'min_df': 2, 'max_df': 0.95}&#10;    ]&#10;    &#10;    best_accuracy = baseline_accuracy&#10;    best_model = baseline_model&#10;    best_vectorizer = vectorizer&#10;    best_config = &quot;baseline&quot;&#10;    &#10;    for i, config in enumerate(tfidf_configs):&#10;        print(f&quot;\nTesting TF-IDF config {i+1}: {config}&quot;)&#10;        &#10;        try:&#10;            # Tạo vectorizer mới&#10;            new_vectorizer = TfidfVectorizer(**config)&#10;            X_new = new_vectorizer.fit_transform(df[f&quot;{feature_column}_processed&quot;])&#10;            &#10;            # Chia lại dữ liệu với cùng random_state&#10;            X_train_new, X_temp_new, y_train_new, y_temp_new = train_test_split(&#10;                X_new, df[f&quot;{target_column}_encoded&quot;], &#10;                test_size=0.2, &#10;                random_state=42, &#10;                stratify=df[f&quot;{target_column}_encoded&quot;]&#10;            )&#10;            &#10;            X_val_new, X_test_new, y_val_new, y_test_new = train_test_split(&#10;                X_temp_new, y_temp_new, &#10;                test_size=0.5, &#10;                random_state=42, &#10;                stratify=y_temp_new&#10;            )&#10;            &#10;            # Test với Logistic Regression đơn giản&#10;            lr_model = LogisticRegression(&#10;                C=1.0,&#10;                random_state=42,&#10;                max_iter=1000,&#10;                class_weight='balanced',&#10;                solver='lbfgs'&#10;            )&#10;            lr_model.fit(X_train_new, y_train_new)&#10;            val_pred = lr_model.predict(X_val_new)&#10;            val_accuracy = accuracy_score(y_val_new, val_pred)&#10;            &#10;            print(f&quot;  LR Validation accuracy: {val_accuracy:.4f}&quot;)&#10;            &#10;            # Test với Random Forest&#10;            rf_model = RandomForestClassifier(&#10;                n_estimators=100,  # Giảm xuống để tránh overfitting&#10;                max_depth=15,      # Giảm depth&#10;                min_samples_split=10,&#10;                min_samples_leaf=5,&#10;                class_weight='balanced',&#10;                random_state=42,&#10;                n_jobs=-1&#10;            )&#10;            rf_model.fit(X_train_new, y_train_new)&#10;            rf_val_pred = rf_model.predict(X_val_new)&#10;            rf_val_accuracy = accuracy_score(y_val_new, rf_val_pred)&#10;            &#10;            print(f&quot;  RF Validation accuracy: {rf_val_accuracy:.4f}&quot;)&#10;            &#10;            # Chọn model tốt hơn&#10;            if lr_model and val_accuracy &gt; best_accuracy:&#10;                best_accuracy = val_accuracy&#10;                best_model = lr_model&#10;                best_vectorizer = new_vectorizer&#10;                best_config = f&quot;LR + TF-IDF config {i+1}&quot;&#10;                best_X_test = X_test_new&#10;                best_y_test = y_test_new&#10;                &#10;            if rf_model and rf_val_accuracy &gt; best_accuracy:&#10;                best_accuracy = rf_val_accuracy&#10;                best_model = rf_model&#10;                best_vectorizer = new_vectorizer&#10;                best_config = f&quot;RF + TF-IDF config {i+1}&quot;&#10;                best_X_test = X_test_new&#10;                best_y_test = y_test_new&#10;                &#10;        except Exception as e:&#10;            print(f&quot;  Error with config {i+1}: {e}&quot;)&#10;            continue&#10;    &#10;    # 3. HyperParameter Tuning cho model tốt nhất (nếu không phải baseline)&#10;    if best_config != &quot;baseline&quot;:&#10;        print(f&quot;\nFine-tuning best model: {best_config}&quot;)&#10;        &#10;        if &quot;LR&quot; in best_config:&#10;            # Grid search cho Logistic Regression&#10;            param_grid = {&#10;                'C': [0.5, 1.0, 2.0, 5.0],&#10;                'solver': ['lbfgs', 'liblinear']&#10;            }&#10;            &#10;            grid_search = GridSearchCV(&#10;                LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000),&#10;                param_grid,&#10;                cv=3,&#10;                scoring='accuracy',&#10;                n_jobs=-1&#10;            )&#10;            &#10;            # Sử dụng data từ best config&#10;            X_train_best = best_vectorizer.fit_transform(df[f&quot;{feature_column}_processed&quot;])&#10;            X_train_split, X_temp_split, y_train_split, y_temp_split = train_test_split(&#10;                X_train_best, df[f&quot;{target_column}_encoded&quot;], &#10;                test_size=0.2, &#10;                random_state=42, &#10;                stratify=df[f&quot;{target_column}_encoded&quot;]&#10;            )&#10;            &#10;            grid_search.fit(X_train_split, y_train_split)&#10;            print(f&quot;  Best parameters: {grid_search.best_params_}&quot;)&#10;            print(f&quot;  Best CV score: {grid_search.best_score_:.4f}&quot;)&#10;            &#10;            # Cập nhật model tốt nhất&#10;            best_model = grid_search.best_estimator_&#10;            best_model.fit(X_train_split, y_train_split)&#10;    &#10;    # 4. ĐÁNH GIÁ CUỐI CÙNG&#10;    print(f&quot;\n&quot; + &quot;=&quot;*60)&#10;    print(&quot;FINAL RESULTS&quot;)&#10;    print(&quot;=&quot;*60)&#10;    &#10;    if best_config == &quot;baseline&quot;:&#10;        # Sử dụng data gốc cho baseline&#10;        final_test_pred = best_model.predict(X_test)&#10;        final_test_accuracy = accuracy_score(y_test, final_test_pred)&#10;        vectorizer = vectorizer  # Giữ vectorizer gốc&#10;    else:&#10;        # Sử dụng data từ best config&#10;        final_test_pred = best_model.predict(best_X_test)&#10;        final_test_accuracy = accuracy_score(best_y_test, final_test_pred)&#10;        vectorizer = best_vectorizer  # Cập nhật vectorizer&#10;    &#10;    print(f&quot;Best configuration: {best_config}&quot;)&#10;    print(f&quot;Validation accuracy: {best_accuracy:.4f}&quot;)&#10;    print(f&quot;Test accuracy: {final_test_accuracy:.4f}&quot;)&#10;    &#10;    if final_test_accuracy &gt; 0.694:&#10;        improvement = final_test_accuracy - 0.694&#10;        improvement_pct = (improvement / 0.694) * 100&#10;        print(f&quot;Improvement: +{improvement:.4f} (+{improvement_pct:.1f}%)&quot;)&#10;    else:&#10;        decline = 0.694 - final_test_accuracy&#10;        decline_pct = (decline / 0.694) * 100&#10;        print(f&quot;Decline: -{decline:.4f} (-{decline_pct:.1f}%)&quot;)&#10;    &#10;    # Sử dụng model tốt nhất cho prediction&#10;    model = best_model&#10;&#10;    # 6. CẬP NHẬT PREDICTION FUNCTION&#10;    def predict_disease_group(input_data, md, vtr, ecr, selector=None):&#10;        ip_processed = process_text(input_data)&#10;        ip_vec = vtr.transform([ip_processed])&#10;&#10;        if selector:&#10;            ip_vec = selector.transform(ip_vec)&#10;&#10;        predicted_label_index = md.predict(ip_vec)[0]&#10;        predicted_group = ecr.inverse_transform([predicted_label_index])[0]&#10;&#10;        # Get prediction probability for confidence&#10;        probabilities = md.predict_proba(ip_vec)[0]&#10;        confidence = max(probabilities)&#10;&#10;        return predicted_group, confidence&#10;&#10;    def show_group_result(input_data, result, confidence):&#10;        print(f&quot;Mô tả triệu chứng: {input_data}&quot;)&#10;        print(f&quot;Dự đoán nhóm bệnh: {result}&quot;)&#10;        print(f&quot;Độ tin cậy: {confidence:.2%}&quot;)&#10;        print(&quot;-&quot; * 50)&#10;&#10;    # Test with some sample predictions&#10;    print(&quot;\n&quot; + &quot;=&quot;*60)&#10;    print(&quot;TESTING OPTIMIZED DISEASE GROUP PREDICTION&quot;)&#10;    print(&quot;=&quot;*60)&#10;&#10;    # Interactive testing với model đã tối ưu&#10;    while True:&#10;        user_input = input(&quot;\nNhập mô tả triệu chứng (hoặc 'quit' để thoát): &quot;)&#10;        if user_input.lower() in ['quit', 'exit', 'thoát']:&#10;            break&#10;&#10;        predicted_group, confidence = predict_disease_group(&#10;            user_input, model, vectorizer, label_encoder,&#10;            selector=feature_selector if 'feature_selector' in locals() else None&#10;        )&#10;        show_group_result(user_input, predicted_group, confidence)&#10;&#10;        # Show top 3 probable groups với model tối ưu&#10;        ip_processed = process_text(user_input)&#10;        ip_vec = vectorizer.transform([ip_processed])&#10;&#10;        if 'feature_selector' in locals():&#10;            ip_vec = feature_selector.transform(ip_vec)&#10;&#10;        probabilities = model.predict_proba(ip_vec)[0]&#10;        top_indices = probabilities.argsort()[-3:][::-1]&#10;&#10;        print(&quot;Top 3 nhóm bệnh có khả năng cao nhất:&quot;)&#10;        for i, idx in enumerate(top_indices, 1):&#10;            group_name = label_encoder.inverse_transform([idx])[0]&#10;            prob = probabilities[idx]&#10;            print(f&quot;{i}. {group_name}: {prob:.2%}&quot;)&#10;&#10;    print(&quot;\nCảm ơn bạn đã sử dụng hệ thống dự đoán nhóm bệnh tối ưu!&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Main_1.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Main_1.py" />
              <option name="originalContent" value="import re&#10;import requests&#10;from sklearn.feature_extraction.text import TfidfVectorizer&#10;from underthesea import word_tokenize&#10;import pandas as pd&#10;from sklearn.preprocessing import LabelEncoder&#10;from sklearn.linear_model import LogisticRegression&#10;from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix&#10;from sklearn.model_selection import train_test_split&#10;from sklearn.utils.class_weight import compute_class_weight&#10;import numpy as np&#10;import matplotlib.pyplot as plt&#10;import seaborn as sns&#10;from collections import Counter&#10;&#10;# Load dataset&#10;def load_data(path):&#10;    &quot;&quot;&quot;Load dataset from given path&quot;&quot;&quot;&#10;    return pd.read_csv(path)&#10;&#10;# Encode labels&#10;def encode_labels(data, column_name, encoded_label_colname, encoder):&#10;    &quot;&quot;&quot;Encode categorical labels to numerical values&quot;&quot;&quot;&#10;    data[encoded_label_colname] = encoder.fit_transform(data[column_name])&#10;    return data&#10;&#10;# Get Vietnamese stopwords from GitHub&#10;def get_vietnamese_stopwords():&#10;    &quot;&quot;&quot;Fetch Vietnamese stopwords from GitHub repository&quot;&quot;&quot;&#10;    try:&#10;        response = requests.get(&quot;https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt&quot;)&#10;        return set(response.text.splitlines())&#10;    except Exception as e:&#10;        print(f&quot;Error fetching stopwords: {e}&quot;)&#10;        return set()&#10;&#10;# Trực quan hóa dữ liệu thô&#10;def visualize_raw_data(df, target_col, feature_col):&#10;    &quot;&quot;&quot;Visualize raw data distribution and characteristics&quot;&quot;&quot;&#10;    plt.style.use('seaborn-v0_8')&#10;    fig, axes = plt.subplots(2, 2, figsize=(15, 12))&#10;&#10;    # 1. Phân bố số lượng mẫu theo nhóm bệnh&#10;    disease_counts = df[target_col].value_counts()&#10;    axes[0, 0].barh(range(len(disease_counts)), disease_counts.values)&#10;    axes[0, 0].set_yticks(range(len(disease_counts)))&#10;    axes[0, 0].set_yticklabels(disease_counts.index, fontsize=8)&#10;    axes[0, 0].set_xlabel('Số lượng mẫu')&#10;    axes[0, 0].set_title('Phân bố dữ liệu theo nhóm bệnh')&#10;    axes[0, 0].grid(True, alpha=0.3)&#10;    plt.tight_layout()&#10;    plt.show()&#10;&#10;&#10;&#10;# Vẽ confusion matrix&#10;def plot_confusion_matrix(y_true, y_pred, label_encoder, title=&quot;Confusion Matrix&quot;):&#10;    &quot;&quot;&quot;Plot confusion matrix with proper labels&quot;&quot;&quot;&#10;    # Tính confusion matrix&#10;    cm = confusion_matrix(y_true, y_pred)&#10;&#10;    # Lấy tên các nhóm bệnh&#10;    class_names = [label_encoder.inverse_transform([i])[0] for i in range(len(label_encoder.classes_))]&#10;&#10;    # Vẽ confusion matrix&#10;    plt.figure(figsize=(12, 10))&#10;    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',&#10;                xticklabels=class_names, yticklabels=class_names)&#10;&#10;    plt.title(title, fontsize=16, fontweight='bold')&#10;    plt.xlabel('Dự đoán', fontsize=12)&#10;    plt.ylabel('Thực tế', fontsize=12)&#10;    plt.xticks(rotation=45, ha='right')&#10;    plt.yticks(rotation=0)&#10;    plt.tight_layout()&#10;    plt.show()&#10;&#10;    # In thống kê confusion matrix&#10;    print(f&quot;\nThống kê {title}:&quot;)&#10;    print(f&quot;Tổng số mẫu: {cm.sum()}&quot;)&#10;    print(f&quot;Dự đoán đúng: {np.trace(cm)} ({np.trace(cm)/cm.sum()*100:.1f}%)&quot;)&#10;    print(f&quot;Dự đoán sai: {cm.sum() - np.trace(cm)} ({(cm.sum() - np.trace(cm))/cm.sum()*100:.1f}%)&quot;)&#10;&#10;# Tiền xử lý văn bản&#10;def preprocess_text(text, stopwords):&#10;    &quot;&quot;&quot;Preprocess the input text by removing special characters, converting to lowercase,&#10;    tokenizing, and removing stopwords.&quot;&quot;&quot;&#10;    if isinstance(text, str):&#10;        # Loại bỏ kí tự đặc biệt&#10;        text = re.sub(r'[^\w\s]', '', text)&#10;        # Chuyển text về chữ thường&#10;        text = text.lower()&#10;        # Tách từ bằng underthesea&#10;        text_tokenized = word_tokenize(text, format=&quot;text&quot;)&#10;        # Loại bỏ stopwords&#10;        words = text_tokenized.split()&#10;        filtered_words = [word for word in words if word not in stopwords]&#10;        return &quot; &quot;.join(filtered_words)&#10;    return &quot;&quot;&#10;&#10;# Làm sạch dataset&#10;def process_dataset(data, label_col, feature_col, processed_feature_col, stopwords):&#10;    &quot;&quot;&quot;Clean and preprocess the dataset by removing duplicates, handling missing values,&#10;    and processing the text data.&quot;&quot;&quot;&#10;    # Loại bỏ trùng lắp&#10;    data.drop_duplicates(inplace=True)&#10;    # Loại bỏ giá trị bị thiếu&#10;    data = data.dropna(subset=[feature_col, label_col])&#10;    # Xử lý text trong cột feature&#10;    data[processed_feature_col] = data[feature_col].apply(lambda x: preprocess_text(x, stopwords))&#10;    return data&#10;&#10;# Tách dữ liệu thành 3 tập train/validation/test (80% - 10% - 10%)&#10;def split_data(X, y, test_size=0.2, val_size=0.5, random_state=42):&#10;    &quot;&quot;&quot;Split the data into training, validation, and test sets.&quot;&quot;&quot;&#10;    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)&#10;    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_size, random_state=random_state, stratify=y_temp)&#10;    return X_train, X_val, X_test, y_train, y_val, y_test&#10;&#10;# Huấn luyện mô hình&#10;def train_model(X_train, y_train):&#10;    &quot;&quot;&quot;Train the logistic regression model with the given training data.&quot;&quot;&quot;&#10;    model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')&#10;    model.fit(X_train, y_train)&#10;    return model&#10;&#10;# Đánh giá model trên tập test&#10;def evaluate_model(model, X_test, y_test, label_encoder):&#10;    &quot;&quot;&quot;Evaluate the trained model on the test set and print the accuracy and F1 score.&quot;&quot;&quot;&#10;    y_test_pred = model.predict(X_test)&#10;    test_accuracy = accuracy_score(y_test, y_test_pred)&#10;    test_f1 = f1_score(y_test, y_test_pred, average='macro')&#10;    print(f&quot;Hiệu suất trên tập test của model:&quot;)&#10;    print(f&quot;Độ chính xác: {test_accuracy:.4f}&quot;)&#10;    print(f&quot;F1 Score (Macro): {test_f1:.4f}&quot;)&#10;    return test_accuracy, test_f1&#10;&#10;# Dự đoán nhóm bệnh từ mô tả&#10;def predict_disease_group(input_text, model, vectorizer, label_encoder, stopwords):&#10;    &quot;&quot;&quot;Predict the disease group for the given input text using the trained model.&quot;&quot;&quot;&#10;    processed_text = preprocess_text(input_text, stopwords)&#10;    text_vector = vectorizer.transform([processed_text])&#10;    predicted_label_index = model.predict(text_vector)[0]&#10;    predicted_group = label_encoder.inverse_transform([predicted_label_index])[0]&#10;    probabilities = model.predict_proba(text_vector)[0]&#10;    confidence = max(probabilities)&#10;    return predicted_group, confidence, probabilities&#10;&#10;# Thử nghiệm&#10;def testing(model, vectorizer, label_encoder, stopwords):&#10;    &quot;&quot;&quot;Interactive testing function to predict disease groups based on user input.&quot;&quot;&quot;&#10;    print(&quot;\n&quot; + &quot;=&quot;*60)&#10;    while True:&#10;        user_input = input(&quot;\nNhập mô tả triệu chứng (hoặc 'quit' để thoát): &quot;)&#10;        if user_input.lower() in ['quit', 'exit', 'thoát']:&#10;            break&#10;        predicted_group, confidence, probabilities = predict_disease_group(user_input, model, vectorizer, label_encoder, stopwords)&#10;        top_indices = probabilities.argsort()[-3:][::-1]&#10;        print(&quot;Top 3 nhóm bệnh bạn có khả năng mắc cao nhất:&quot;)&#10;        for i, idx in enumerate(top_indices, 1):&#10;            group_name = label_encoder.inverse_transform([idx])[0]&#10;            prob = probabilities[idx]&#10;            print(f&quot;{i}. {group_name}: \nXác suất: {prob:.2%}&quot;)&#10;    print(&quot;\nCảm ơn bạn đã sử dụng hệ thống dự đoán nhóm bệnh!&quot;)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    TARGET_COLUMN = 'DiseaseCategory'&#10;    FEATURE_COLUMN = 'Question'&#10;    PROCESSED_FEATURE_COLUMN = f'{FEATURE_COLUMN}_processed'&#10;    ENCODED_TARGET_COLUMN = f'{TARGET_COLUMN}_encoded'&#10;&#10;    # Load dữ liệu&#10;    df = load_data(&quot;hf://datasets/joon985/ViMedical_Disease_Category/ViMedicalDiseaseCategory.csv&quot;)&#10;&#10;    # Trực quan hóa dữ liệu thô TRƯỚC khi xử lý&#10;    print(&quot;Trực quan hóa dữ liệu thô...&quot;)&#10;    visualize_raw_data(df, TARGET_COLUMN, FEATURE_COLUMN)&#10;&#10;    # Tiếp tục xử lý dữ liệu&#10;    vietnamese_stopwords = get_vietnamese_stopwords()&#10;    label_encoder = LabelEncoder()&#10;    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))&#10;    df = encode_labels(df, TARGET_COLUMN, ENCODED_TARGET_COLUMN, label_encoder)&#10;    df = process_dataset(df, TARGET_COLUMN, FEATURE_COLUMN, PROCESSED_FEATURE_COLUMN, vietnamese_stopwords)&#10;    X_vec = vectorizer.fit_transform(df[PROCESSED_FEATURE_COLUMN])&#10;    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_vec, df[ENCODED_TARGET_COLUMN])&#10;    classes = np.unique(y_train)&#10;    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)&#10;    class_weight_dict = dict(zip(classes, class_weights))&#10;&#10;    # Huấn luyện model&#10;    model = train_model(X_train, y_train)&#10;&#10;    # Đánh giá model&#10;    test_accuracy, test_f1 = evaluate_model(model, X_test, y_test, label_encoder)&#10;&#10;    # Vẽ confusion matrix SAU khi train&#10;    print(&quot;\nVẽ Confusion Matrix...&quot;)&#10;    y_test_pred = model.predict(X_test)&#10;    plot_confusion_matrix(y_test, y_test_pred, label_encoder, &quot;Confusion Matrix - Test Set&quot;)&#10;&#10;    # Test tương tác&#10;    testing(model, vectorizer, label_encoder, vietnamese_stopwords)&#10;" />
              <option name="updatedContent" value="import re&#10;import requests&#10;from sklearn.feature_extraction.text import TfidfVectorizer&#10;from underthesea import word_tokenize&#10;import pandas as pd&#10;from sklearn.preprocessing import LabelEncoder&#10;from sklearn.linear_model import LogisticRegression&#10;from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix&#10;from sklearn.model_selection import train_test_split&#10;from sklearn.utils.class_weight import compute_class_weight&#10;import numpy as np&#10;import matplotlib.pyplot as plt&#10;import seaborn as sns&#10;from collections import Counter&#10;&#10;# Load dataset&#10;def load_data(path):&#10;    &quot;&quot;&quot;Load dataset from given path&quot;&quot;&quot;&#10;    return pd.read_csv(path)&#10;&#10;# Encode labels&#10;def encode_labels(data, column_name, encoded_label_colname, encoder):&#10;    &quot;&quot;&quot;Encode categorical labels to numerical values&quot;&quot;&quot;&#10;    data[encoded_label_colname] = encoder.fit_transform(data[column_name])&#10;    return data&#10;&#10;# Get Vietnamese stopwords from GitHub&#10;def get_vietnamese_stopwords():&#10;    &quot;&quot;&quot;Fetch Vietnamese stopwords from GitHub repository&quot;&quot;&quot;&#10;    try:&#10;        response = requests.get(&quot;https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt&quot;)&#10;        return set(response.text.splitlines())&#10;    except Exception as e:&#10;        print(f&quot;Error fetching stopwords: {e}&quot;)&#10;        return set()&#10;&#10;# Trực quan hóa dữ liệu thô&#10;def visualize_raw_data(df, target_col, feature_col):&#10;    &quot;&quot;&quot;Visualize raw data distribution and characteristics&quot;&quot;&quot;&#10;    plt.style.use('seaborn-v0_8')&#10;    fig, ax = plt.subplots(1, 1, figsize=(12, 8))&#10;    &#10;    # Phân bố số lượng mẫu theo nhóm bệnh&#10;    disease_counts = df[target_col].value_counts()&#10;    bars = ax.barh(range(len(disease_counts)), disease_counts.values, color='skyblue', edgecolor='navy', alpha=0.7)&#10;    ax.set_yticks(range(len(disease_counts)))&#10;    ax.set_yticklabels(disease_counts.index, fontsize=10)&#10;    ax.set_xlabel('Số lượng mẫu', fontsize=12)&#10;    ax.set_title('Phân bố dữ liệu theo nhóm bệnh', fontsize=14, fontweight='bold')&#10;    ax.grid(True, alpha=0.3, axis='x')&#10;    &#10;    # Thêm số liệu trên các thanh&#10;    for i, bar in enumerate(bars):&#10;        width = bar.get_width()&#10;        ax.text(width + 10, bar.get_y() + bar.get_height()/2, &#10;                f'{int(width)}', ha='left', va='center', fontsize=9)&#10;    &#10;    plt.tight_layout()&#10;    plt.show()&#10;    &#10;    # In thống kê cơ bản&#10;    print(&quot;\n&quot; + &quot;=&quot;*50)&#10;    print(&quot;THỐNG KÊ PHÂN BỐ DỮ LIỆU&quot;)&#10;    print(&quot;=&quot;*50)&#10;    print(f&quot;Tổng số mẫu: {len(df)}&quot;)&#10;    print(f&quot;Số nhóm bệnh: {df[target_col].nunique()}&quot;)&#10;    print(f&quot;Nhóm bệnh có nhiều mẫu nhất: {disease_counts.index[0]} ({disease_counts.iloc[0]} mẫu)&quot;)&#10;    print(f&quot;Nhóm bệnh có ít mẫu nhất: {disease_counts.index[-1]} ({disease_counts.iloc[-1]} mẫu)&quot;)&#10;    print(f&quot;Tỷ lệ imbalance: {disease_counts.iloc[0]/disease_counts.iloc[-1]:.2f}:1&quot;)&#10;&#10;# Vẽ confusion matrix&#10;def plot_confusion_matrix(y_true, y_pred, label_encoder, title=&quot;Confusion Matrix&quot;):&#10;    &quot;&quot;&quot;Plot confusion matrix with proper labels&quot;&quot;&quot;&#10;    # Tính confusion matrix&#10;    cm = confusion_matrix(y_true, y_pred)&#10;&#10;    # Lấy tên các nhóm bệnh&#10;    class_names = [label_encoder.inverse_transform([i])[0] for i in range(len(label_encoder.classes_))]&#10;&#10;    # Vẽ confusion matrix&#10;    plt.figure(figsize=(12, 10))&#10;    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',&#10;                xticklabels=class_names, yticklabels=class_names)&#10;&#10;    plt.title(title, fontsize=16, fontweight='bold')&#10;    plt.xlabel('Dự đoán', fontsize=12)&#10;    plt.ylabel('Thực tế', fontsize=12)&#10;    plt.xticks(rotation=45, ha='right')&#10;    plt.yticks(rotation=0)&#10;    plt.tight_layout()&#10;    plt.show()&#10;&#10;    # In thống kê confusion matrix&#10;    print(f&quot;\nThống kê {title}:&quot;)&#10;    print(f&quot;Tổng số mẫu: {cm.sum()}&quot;)&#10;    print(f&quot;Dự đoán đúng: {np.trace(cm)} ({np.trace(cm)/cm.sum()*100:.1f}%)&quot;)&#10;    print(f&quot;Dự đoán sai: {cm.sum() - np.trace(cm)} ({(cm.sum() - np.trace(cm))/cm.sum()*100:.1f}%)&quot;)&#10;&#10;# Tiền xử lý văn bản&#10;def preprocess_text(text, stopwords):&#10;    &quot;&quot;&quot;Preprocess the input text by removing special characters, converting to lowercase,&#10;    tokenizing, and removing stopwords.&quot;&quot;&quot;&#10;    if isinstance(text, str):&#10;        # Loại bỏ kí tự đặc biệt&#10;        text = re.sub(r'[^\w\s]', '', text)&#10;        # Chuyển text về chữ thường&#10;        text = text.lower()&#10;        # Tách từ bằng underthesea&#10;        text_tokenized = word_tokenize(text, format=&quot;text&quot;)&#10;        # Loại bỏ stopwords&#10;        words = text_tokenized.split()&#10;        filtered_words = [word for word in words if word not in stopwords]&#10;        return &quot; &quot;.join(filtered_words)&#10;    return &quot;&quot;&#10;&#10;# Làm sạch dataset&#10;def process_dataset(data, label_col, feature_col, processed_feature_col, stopwords):&#10;    &quot;&quot;&quot;Clean and preprocess the dataset by removing duplicates, handling missing values,&#10;    and processing the text data.&quot;&quot;&quot;&#10;    # Loại bỏ trùng lắp&#10;    data.drop_duplicates(inplace=True)&#10;    # Loại bỏ giá trị bị thiếu&#10;    data = data.dropna(subset=[feature_col, label_col])&#10;    # Xử lý text trong cột feature&#10;    data[processed_feature_col] = data[feature_col].apply(lambda x: preprocess_text(x, stopwords))&#10;    return data&#10;&#10;# Tách dữ liệu thành 3 tập train/validation/test (80% - 10% - 10%)&#10;def split_data(X, y, test_size=0.2, val_size=0.5, random_state=42):&#10;    &quot;&quot;&quot;Split the data into training, validation, and test sets.&quot;&quot;&quot;&#10;    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)&#10;    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_size, random_state=random_state, stratify=y_temp)&#10;    return X_train, X_val, X_test, y_train, y_val, y_test&#10;&#10;# Huấn luyện mô hình&#10;def train_model(X_train, y_train):&#10;    &quot;&quot;&quot;Train the logistic regression model with the given training data.&quot;&quot;&quot;&#10;    model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')&#10;    model.fit(X_train, y_train)&#10;    return model&#10;&#10;# Đánh giá model trên tập test&#10;def evaluate_model(model, X_test, y_test, label_encoder):&#10;    &quot;&quot;&quot;Evaluate the trained model on the test set and print the accuracy and F1 score.&quot;&quot;&quot;&#10;    y_test_pred = model.predict(X_test)&#10;    test_accuracy = accuracy_score(y_test, y_test_pred)&#10;    test_f1 = f1_score(y_test, y_test_pred, average='macro')&#10;    print(f&quot;Hiệu suất trên tập test của model:&quot;)&#10;    print(f&quot;Độ chính xác: {test_accuracy:.4f}&quot;)&#10;    print(f&quot;F1 Score (Macro): {test_f1:.4f}&quot;)&#10;    return test_accuracy, test_f1&#10;&#10;# Dự đoán nhóm bệnh từ mô tả&#10;def predict_disease_group(input_text, model, vectorizer, label_encoder, stopwords):&#10;    &quot;&quot;&quot;Predict the disease group for the given input text using the trained model.&quot;&quot;&quot;&#10;    processed_text = preprocess_text(input_text, stopwords)&#10;    text_vector = vectorizer.transform([processed_text])&#10;    predicted_label_index = model.predict(text_vector)[0]&#10;    predicted_group = label_encoder.inverse_transform([predicted_label_index])[0]&#10;    probabilities = model.predict_proba(text_vector)[0]&#10;    confidence = max(probabilities)&#10;    return predicted_group, confidence, probabilities&#10;&#10;# Thử nghiệm&#10;def testing(model, vectorizer, label_encoder, stopwords):&#10;    &quot;&quot;&quot;Interactive testing function to predict disease groups based on user input.&quot;&quot;&quot;&#10;    print(&quot;\n&quot; + &quot;=&quot;*60)&#10;    while True:&#10;        user_input = input(&quot;\nNhập mô tả triệu chứng (hoặc 'quit' để thoát): &quot;)&#10;        if user_input.lower() in ['quit', 'exit', 'thoát']:&#10;            break&#10;        predicted_group, confidence, probabilities = predict_disease_group(user_input, model, vectorizer, label_encoder, stopwords)&#10;        top_indices = probabilities.argsort()[-3:][::-1]&#10;        print(&quot;Top 3 nhóm bệnh bạn có khả năng mắc cao nhất:&quot;)&#10;        for i, idx in enumerate(top_indices, 1):&#10;            group_name = label_encoder.inverse_transform([idx])[0]&#10;            prob = probabilities[idx]&#10;            print(f&quot;{i}. {group_name}: \nXác suất: {prob:.2%}&quot;)&#10;    print(&quot;\nCảm ơn bạn đã sử dụng hệ thống dự đoán nhóm bệnh!&quot;)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    TARGET_COLUMN = 'DiseaseCategory'&#10;    FEATURE_COLUMN = 'Question'&#10;    PROCESSED_FEATURE_COLUMN = f'{FEATURE_COLUMN}_processed'&#10;    ENCODED_TARGET_COLUMN = f'{TARGET_COLUMN}_encoded'&#10;&#10;    # Load dữ liệu&#10;    df = load_data(&quot;hf://datasets/joon985/ViMedical_Disease_Category/ViMedicalDiseaseCategory.csv&quot;)&#10;&#10;    # Trực quan hóa dữ liệu thô TRƯỚC khi xử lý&#10;    print(&quot;Trực quan hóa dữ liệu thô...&quot;)&#10;    visualize_raw_data(df, TARGET_COLUMN, FEATURE_COLUMN)&#10;&#10;    # Tiếp tục xử lý dữ liệu&#10;    vietnamese_stopwords = get_vietnamese_stopwords()&#10;    label_encoder = LabelEncoder()&#10;    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))&#10;    df = encode_labels(df, TARGET_COLUMN, ENCODED_TARGET_COLUMN, label_encoder)&#10;    df = process_dataset(df, TARGET_COLUMN, FEATURE_COLUMN, PROCESSED_FEATURE_COLUMN, vietnamese_stopwords)&#10;    X_vec = vectorizer.fit_transform(df[PROCESSED_FEATURE_COLUMN])&#10;    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_vec, df[ENCODED_TARGET_COLUMN])&#10;    classes = np.unique(y_train)&#10;    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)&#10;    class_weight_dict = dict(zip(classes, class_weights))&#10;&#10;    # Huấn luyện model&#10;    model = train_model(X_train, y_train)&#10;&#10;    # Đánh giá model&#10;    test_accuracy, test_f1 = evaluate_model(model, X_test, y_test, label_encoder)&#10;&#10;    # Vẽ confusion matrix SAU khi train&#10;    print(&quot;\nVẽ Confusion Matrix...&quot;)&#10;    y_test_pred = model.predict(X_test)&#10;    plot_confusion_matrix(y_test, y_test_pred, label_encoder, &quot;Confusion Matrix - Test Set&quot;)&#10;&#10;    # Test tương tác&#10;    testing(model, vectorizer, label_encoder, vietnamese_stopwords)" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>